# -*- coding: utf-8 -*-
"""Mursalin_CV_Ass2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19Pl3Gl-oZ_2V8enBIR80lQ1goAFxIka_

# Mursalin -  (Image Captioning Project)

# Preliminary and Dataset Loading
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# #The output display will be suppressed by the previous command
# !wget https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip
# !wget https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip
# !unzip Flickr8k_Dataset.zip
# !unzip Flickr8k_text.zip

"""Now we will load some libraries that will be used ahead."""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import pickle
import os
from IPython.display import Image, display
from keras.applications.resnet50 import ResNet50
from keras.optimizers import Adam
from keras.layers import Dense, Flatten,Input, Convolution2D, Dropout, LSTM, TimeDistributed, Embedding, Bidirectional, Activation, RepeatVector,Concatenate
from keras.models import Sequential, Model
from keras.utils import np_utils
import random
from keras.preprocessing import image, sequence

"""Data Loading.
The Flickr_8k.token.txt contains the name of each image followed by “#{digit}” followed by its caption. The
digit refers to the fact that for each image there are multiple captions. 

"""

images_path = 'Flicker8k_Dataset/' #Folder

captions = open('Flickr8k.token.txt', 'r').read().split("\n")[:-1]
x_train = open('Flickr_8k.trainImages.txt', 'r').read().split("\n")[:-1]
x_val = open('Flickr_8k.devImages.txt', 'r').read().split("\n")[:-1]
x_test = open('Flickr_8k.testImages.txt', 'r').read().split("\n")[:-1]

"""Resolving the captions and images"""

tokens = {}

for i in range(len(captions)-1):
    temp = captions[i].split("#")
    if temp[0] in tokens:
        tokens[temp[0]].append("<START> "+ temp[1][2:] + " <END>")
    else:
        tokens[temp[0]] = ["<START> "+ temp[1][2:] + " <END>"]

"""Generate Train/Test Dataset files for loading in Pandas. 
We will make a Pandas DataFrame for ease when making the model, hence we will create a CSV File from the Datasets.
"""

# Creating train, test and validation dataset files with header 
# as 'image_id' and 'captions'.
# Going to be used for making the Pandas Dataframe
train_dataset = open('flickr_8k_train_dataset.txt','wb')
val_dataset = open('flickr_8k_val_dataset.txt','wb')
test_dataset = open('  flickr_8k_test_dataset.txt','wb')

train_dataset.write(b"image_id\tcaptions\n")
val_dataset.write(b"image_id\tcaptions\n")
test_dataset.write(b"image_id\tcaptions\n")

# Populating the above created files for train, test and 
# validation dataset with image ids and captions for each 
# of these images
for img in x_train:
    if img == '':
        continue
    for capt in tokens[img]:
        train_dataset.write((img+"\t"+capt+"\n").encode())
        train_dataset.flush()
train_dataset.close()

for img in x_test:
    if img == '':
        continue
    for capt in tokens[img]:
        test_dataset.write((img+"\t"+capt+"\n").encode())
        test_dataset.flush()
test_dataset.close()

for img in x_val:
    if img == '':
        continue
    for capt in tokens[img]:
        val_dataset.write((img+"\t"+capt+"\n").encode())
        val_dataset.flush()
val_dataset.close()

"""# Pre-processing Steps"""

# Pre-trained ResNet 50 is used with last layer removed to get feature vectors
model = ResNet50(include_top=False,weights='imagenet',input_shape=(224,224,3),pooling='avg')

# Image Preprocessing
def preprocessing(img_path):
    im = image.load_img(img_path, target_size=(224,224,3))
    return np.expand_dims(image.img_to_array(im), axis=0)

train_data = {}
for i, val in enumerate(x_train):
    train_data[val] = model.predict(preprocessing(images_path + val)).reshape(2048)

# saving train_data dict to avoid making the variable again
with open( "train_encoded_images.p", "wb" ) as pickle_f:
    pickle.dump(train_data, pickle_f )

"""Making of DataFrame"""

# Loading image and its corresponding caption into a dataframe and then storing values from dataframe into 'ds'
pd_dataset_train = pd.read_csv("flickr_8k_train_dataset.txt", delimiter='\t')
ds = pd_dataset_train.values
print(ds.shape)

# Storing all the captions from ds into a list
sentences = []
for ix in range(ds.shape[0]):
    sentences.append(ds[ix, 1])
    
print(len(sentences))

"""Collect all the words used in captions and vectorize them to be used for training"""

# Splitting each captions stored in 'sentences' and storing them in 'words' as list of list
words = [i.split() for i in sentences]
# Creating a list of all unique words
unique = []
for i in words:
    unique.extend(i)
unique = list(set(unique))

print(len(unique))

vocab_size = len(unique)

# Vectorization
word_2_indices = {val:index for index, val in enumerate(unique)}
indices_2_word = {index:val for index, val in enumerate(unique)}

word_2_indices['UNK'] = 8253
indices_2_word[8253] = 'UNK'

vocab_size = len(word_2_indices.keys())
print(vocab_size)

max_len = 0

for i in sentences:
    i = i.split()
    if len(i) > max_len:
        max_len = len(i)

print(max_len)

padded_sequences, subsequent_words = [], []

for ix in range(ds.shape[0]):
    partial_seqs = []
    next_words = []
    text = ds[ix, 1].split()
    text = [word_2_indices[i] for i in text]
    for i in range(1, len(text)):
        partial_seqs.append(text[:i])
        next_words.append(text[i])
    padded_partial_seqs = sequence.pad_sequences(partial_seqs, max_len, padding='post')

    next_words_1hot = np.zeros([len(next_words), vocab_size], dtype=np.bool)
    
    #Vectorization
    for i,next_word in enumerate(next_words):
        next_words_1hot[i, next_word] = 1
        
    padded_sequences.append(padded_partial_seqs)
    subsequent_words.append(next_words_1hot)
    
padded_sequences = np.asarray(padded_sequences)
subsequent_words = np.asarray(subsequent_words)

print(padded_sequences.shape)
print(subsequent_words.shape)

num_of_images = 3000

captions = np.zeros([0, max_len])
next_words = np.zeros([0, vocab_size])

for ix in range(num_of_images):#img_to_padded_seqs.shape[0]):
    captions = np.concatenate([captions, padded_sequences[ix]])
    next_words = np.concatenate([next_words, subsequent_words[ix]])

np.save("captions.npy", captions)
np.save("next_words.npy", next_words)

print(captions.shape)
print(next_words.shape)

"""Save all variables. Was not working due to insufficient RAM"""

# TEST CODE FOR SAVING ALL VARIABLE STATES INTO A FILE
# import shelve
# filename='/tmp/shelve.out'
# my_shelf = shelve.open(filename,'n') # 'n' for new
# for key in dir():
#     try:
#         my_shelf[key] = globals()[key]
#     except:
#         #
#         # __builtins__, my_shelf, and imported modules can not be shelved.
#         #
#         print('ERROR shelving: {0}'.format(key))
# my_shelf.close()

globals().keys()

with open('train_encoded_images.p', 'rb') as f:
    encoded_images = pickle.load(f, encoding="bytes")

imgs = []

for ix in range(ds.shape[0]):
    if ds[ix, 0] in encoded_images.keys():
#         print(ix, encoded_images[ds[ix, 0].encode()])
        imgs.append(list(encoded_images[ds[ix, 0]]))
print(len(imgs))
imgs = np.asarray(imgs)
print(imgs.shape)

images = []

for ix in range(num_of_images):
    for iy in range(padded_sequences[ix].shape[0]):
        images.append(imgs[ix])
        
images = np.asarray(images)

np.save("images.npy", images)

print(images.shape)

image_names = []

for ix in range(num_of_images):
    for iy in range(padded_sequences[ix].shape[0]):
        image_names.append(ds[ix, 0])
        
image_names = np.asarray(image_names)

np.save("image_names.npy", image_names)

print(len(image_names))

"""# Model"""

# captions = np.load("/content/drive/My Drive/CV_Assignment2/captions.npy")
# next_words = np.load("/content/drive/My Drive/CV_Assignment2/next_words.npy")
captions = np.load("captions.npy")
next_words = np.load("next_words.npy")

print(captions.shape)
print(next_words.shape)

# images = np.load("/content/drive/My Drive/CV_Assignment2/images.npy")
images = np.load("images.npy")
print(images.shape)

# imag = np.load("/content/drive/My Drive/CV_Assignment2/image_names.npy")
imag = np.load("image_names.npy")
print(imag.shape)

embedding_size = 128

image_model = Sequential()

image_model.add(Dense(embedding_size, input_shape=(2048,), activation='relu'))
image_model.add(RepeatVector(max_len))

image_model.summary()

language_model = Sequential()
language_model.add(Embedding(input_dim=vocab_size, output_dim=embedding_size, input_length=max_len))
language_model.add(LSTM(256, return_sequences=True))
language_model.add(TimeDistributed(Dense(embedding_size)))

language_model.summary()

conca = Concatenate()([image_model.output, language_model.output])
x = LSTM(128, return_sequences=True)(conca)
x = LSTM(512, return_sequences=False)(x)
x = Dense(vocab_size)(x)
x = Dropout(0.3)(x)
out = Activation('softmax')(x)
model = Model(inputs=[image_model.input, language_model.input], outputs = out)

# model.load_weights("../input/model_weights.h5")
model.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])
model.summary()

"""Using the Validation and Test Dataset was not possible due to insufficient RAM and Memory. So the 10% of Training set was used for validation."""

# checkpoint_filepath = "/content/drive/My Drive/CV_Assignment2/checkpoint"
# from keras.callbacks import ModelCheckpoint
# model_checkpoint_callback = ModelCheckpoint(
#     filepath=checkpoint_filepath,
#     save_weights_only=True,
#     monitor='accuracy',
#     mode='max',
#     save_best_only=True)
hist = model.fit([images, captions], next_words, batch_size=512, validation_split=0.1, epochs=130)#, callbacks=[model_checkpoint_callback])

"""Save to Google Colab"""

from google.colab import drive
drive.mount('/content/drive')

model.save_weights("/content/drive/My Drive/CV_Assignment2/model_weights_9th_July.h5")
!cp "captions.npy" -r "/content/drive/My Drive/CV_Assignment2/captions.npy"
!cp "next_words.npy" -r "/content/drive/My Drive/CV_Assignment2/next_words.npy"
!cp "image_names.npy" -r "/content/drive/My Drive/CV_Assignment2/image_names.npy"
!cp "captions.npy" -r "/content/drive/My Drive/CV_Assignment2/images.npy"

!ls *.npy

# Load Weights (Run if you don't want to train again)
model.load_weights("/content/drive/My Drive/CV_Assignment2/model_weights_400_epochs.h5")

"""# Predictions"""

def preprocessing(img_path):
    im = image.load_img(img_path, target_size=(224,224,3))
    im = image.img_to_array(im)
    im = np.expand_dims(im, axis=0)
    return im

def get_encoding(model, img):
    image = preprocessing(img)
    pred = model.predict(image).reshape(2048)
    return pred

resnet = ResNet50(include_top=False,weights='imagenet',input_shape=(224,224,3),pooling='avg')

def predict_captions(image):
    start_word = ["<START>"]
    while True:
        par_caps = [word_2_indices[i] for i in start_word]
        par_caps = sequence.pad_sequences([par_caps], maxlen=max_len, padding='post')
        preds = model.predict([np.array([image]), np.array(par_caps)])
        word_pred = indices_2_word[np.argmax(preds[0])]
        start_word.append(word_pred)
        
        if word_pred == "<END>" or len(start_word) > max_len:
            break
            
    return ' '.join(start_word[1:-1])

"""# Results"""

import matplotlib.pyplot as plt
import random
plt.figure(figsize=(10,50))
for i in range (10):
  ind = random.randint(0,len(x_test))
  img = "Flicker8k_Dataset/"+ x_test[ind]
  test_img = get_encoding(resnet, img)
  Argmax_Search = predict_captions(test_img)
  z = image.load_img(img)
  plt.subplot(10,1,i+1)
  plt.imshow(z)
  #display(z)
  print("Generated Caption:\n" + Argmax_Search)
  print ("Actual Caption")
  for j in tokens[x_test[ind]]:
    print(j)

x_train[8]

plt.plot(model.history.history['accuracy'])
plt.xlabel('epochs')
plt.ylabel('accuracy')
plt.title('Training Accuracy')